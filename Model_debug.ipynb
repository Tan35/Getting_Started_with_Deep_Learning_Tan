{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbbf58a-89ed-40c9-9aea-18235daff811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import time, json\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange, repeat\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "# 等于 PreNorm\n",
    "class LayerNormalize(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "# 等于 FeedForward\n",
    "class MLP_Block(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, heads, dim_heads, dropout):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_heads * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_heads ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, mask = None):\n",
    "        # x:[b,n,dim]\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        # get qkv tuple:([b,n,head_num*head_dim],[...],[...])\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        # split q,k,v from [b,n,head_num*head_dim] -> [b,head_num,n,head_dim]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        # transpose(k) * q / sqrt(head_dim) -> [b,head_num,n,n]\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "\n",
    "        # mask value: -inf\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        # softmax normalization -> attention matrix\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        # value * attention matrix -> output\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        # cat all output -> [b, n, head_num*head_dim]\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(LayerNormalize(dim, Attention(dim, heads=heads, dim_heads=dim_heads, dropout=dropout))),\n",
    "                Residual(LayerNormalize(dim, MLP_Block(dim, mlp_dim, dropout=dropout)))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for attention, mlp in self.layers:\n",
    "            x = attention(x, mask=mask)  # go to attention\n",
    "            x = mlp(x)  # go to MLP_Block\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chnls, ratio):\n",
    "        super(SE, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.compress = nn.Conv2d(in_chnls, in_chnls//ratio, 1, 1, 0)\n",
    "        self.excitation = nn.Conv2d(in_chnls//ratio, in_chnls, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.squeeze(x)\n",
    "        out = self.compress(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.excitation(out)\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "\n",
    "class HSINet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HSINet, self).__init__()\n",
    "        self.params = params\n",
    "        net_params = params['net']\n",
    "        data_params = params['data']\n",
    "\n",
    "        num_classes = data_params.get(\"num_classes\", 16)\n",
    "        patch_size = data_params.get(\"patch_size\", 13)\n",
    "        self.spectral_size = data_params.get(\"spectral_size\", 200)\n",
    "\n",
    "        depth = net_params.get(\"depth\", 1)\n",
    "        heads = net_params.get(\"heads\", 8)\n",
    "        mlp_dim = net_params.get(\"mlp_dim\", 8)\n",
    "        dropout = net_params.get(\"dropout\", 0)\n",
    "        conv2d_out = 64\n",
    "        dim = net_params.get(\"dim\", 64)\n",
    "        dim_heads = dim\n",
    "        mlp_head_dim = dim\n",
    "        \n",
    "        image_size = patch_size * patch_size\n",
    "\n",
    "        self.pixel_patch_embedding = nn.Linear(conv2d_out, dim)\n",
    "\n",
    "        self.local_trans_pixel = Transformer(dim=dim, depth=depth, heads=heads, dim_heads=dim_heads, mlp_dim=mlp_dim, dropout=dropout)\n",
    "        self.new_image_size = image_size\n",
    "        self.pixel_pos_embedding = nn.Parameter(torch.randn(1, self.new_image_size+1, dim))\n",
    "        self.pixel_pos_scale = nn.Parameter(torch.ones(1) * 0.01)\n",
    "\n",
    "        self.conv2d_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.spectral_size, out_channels=conv2d_out, kernel_size=(3, 3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(conv2d_out),\n",
    "            nn.ReLU(),\n",
    "            # featuremap 是在这之后加一层channel上的压缩\n",
    "            # nn.Conv2d(in_channels=conv2d_out,out_channels=dim,kernel_size=3,padding=1),\n",
    "            # nn.BatchNorm2d(dim),\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.senet = SE(conv2d_out, 5)\n",
    "\n",
    "        self.cls_token_pixel = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.to_latent_pixel = nn.Identity()\n",
    "\n",
    "        self.mlp_head =nn.Linear(dim, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.mlp_head.weight)\n",
    "        torch.nn.init.normal_(self.mlp_head.bias, std=1e-6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        linear_dim = dim * 2\n",
    "        self.classifier_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, linear_dim),\n",
    "            nn.BatchNorm1d(linear_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(linear_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def encoder_block(self, x):\n",
    "        '''\n",
    "        x: (batch, s, w, h), s=spectral, w=weigth, h=height\n",
    "        '''\n",
    "        x_pixel = x \n",
    "\n",
    "        b, s, w, h = x_pixel.shape\n",
    "        img = w * h\n",
    "        x_pixel = self.conv2d_features(x_pixel)\n",
    "\n",
    "\n",
    "        scale = self.senet(x_pixel)\n",
    "        # print('scale shape is ', scale.shape)\n",
    "        # print('pixel shape is ', x_pixel.shape)\n",
    "        # x_pixel = x_pixel * scale#(batch, image_size, dim)\n",
    "\n",
    "        #1. reshape\n",
    "        x_pixel = rearrange(x_pixel, 'b s w h-> b (w h) s') # (batch, w*h, s)\n",
    "\n",
    "        #2. patch_embedding\n",
    "        # x_pixel = self.pixel_patch_embedding(x_pixel)\n",
    "\n",
    "        #3. local transformer\n",
    "        cls_tokens_pixel = self.cls_token_pixel.expand(x_pixel.shape[0], -1, -1)\n",
    "        x_pixel = torch.cat((cls_tokens_pixel, x_pixel), dim = 1) #[b,image+1,dim]\n",
    "        x_pixel = x_pixel + self.pixel_pos_embedding[:,:] * self.pixel_pos_scale\n",
    "        # x_pixel = x_pixel + self.pixel_pos_embedding[:,:] \n",
    "        # x_pixel = self.dropout(x_pixel)\n",
    "\n",
    "        x_pixel = self.local_trans_pixel(x_pixel) #(batch, image_size+1, dim)\n",
    "\n",
    "        logit_pixel = self.to_latent_pixel(x_pixel[:,0])\n",
    "\n",
    "        logit_x = logit_pixel \n",
    "        reduce_x = torch.mean(x_pixel, dim=1)\n",
    "        \n",
    "        return logit_x, reduce_x\n",
    "\n",
    "    def forward(self, x,left=None,right=None):\n",
    "        '''\n",
    "        x: (batch, s, w, h), s=spectral, w=weigth, h=height\n",
    "\n",
    "        '''\n",
    "        logit_x, _ = self.encoder_block(x)\n",
    "        mean_left, mean_right = None, None\n",
    "        if left is not None and right is not None:\n",
    "            _, mean_left = self.encoder_block(left)\n",
    "            _, mean_right = self.encoder_block(right)\n",
    "\n",
    "        # return  self.mlp_head(logit_x), mean_left, mean_right \n",
    "        return  self.classifier_mlp(logit_x), mean_left, mean_right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c6bd5c-3025-4e0d-a369-51b585892a3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Kernel Shape\n",
       "==================================================================================================================================\n",
       "HSINet                                                  [1, 1200, 13, 13]         [1, 16]                   --\n",
       "├─Sequential: 1-1                                       [1, 1200, 13, 13]         [1, 64, 13, 13]           --\n",
       "│    └─Conv2d: 2-1                                      [1, 1200, 13, 13]         [1, 64, 13, 13]           [3, 3]\n",
       "│    └─BatchNorm2d: 2-2                                 [1, 64, 13, 13]           [1, 64, 13, 13]           --\n",
       "│    └─ReLU: 2-3                                        [1, 64, 13, 13]           [1, 64, 13, 13]           --\n",
       "├─SE: 1-2                                               [1, 64, 13, 13]           [1, 64, 1, 1]             --\n",
       "│    └─AdaptiveAvgPool2d: 2-4                           [1, 64, 13, 13]           [1, 64, 1, 1]             --\n",
       "│    └─Conv2d: 2-5                                      [1, 64, 1, 1]             [1, 12, 1, 1]             [1, 1]\n",
       "│    └─Conv2d: 2-6                                      [1, 12, 1, 1]             [1, 64, 1, 1]             [1, 1]\n",
       "├─Transformer: 1-3                                      [1, 170, 64]              [1, 170, 64]              --\n",
       "│    └─ModuleList: 2-7                                  --                        --                        --\n",
       "│    │    └─ModuleList: 3-1                             --                        --                        --\n",
       "│    │    └─ModuleList: 3-2                             --                        --                        --\n",
       "├─Identity: 1-4                                         [1, 64]                   [1, 64]                   --\n",
       "├─Sequential: 1-5                                       [1, 64]                   [1, 16]                   --\n",
       "│    └─Linear: 2-8                                      [1, 64]                   [1, 128]                  --\n",
       "│    └─BatchNorm1d: 2-9                                 [1, 128]                  [1, 128]                  --\n",
       "│    └─Dropout: 2-10                                    [1, 128]                  [1, 128]                  --\n",
       "│    └─ReLU: 2-11                                       [1, 128]                  [1, 128]                  --\n",
       "│    └─Linear: 2-12                                     [1, 128]                  [1, 16]                   --\n",
       "==================================================================================================================================\n",
       "Total params: 1,377,981\n",
       "Trainable params: 1,377,981\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 117.49\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.81\n",
       "Forward/backward pass size (MB): 11.34\n",
       "Params size (MB): 5.45\n",
       "Estimated Total Size (MB): 17.60\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_param = './params/indian_diffusion.json'\n",
    "with open(path_param, 'r') as fin:\n",
    "    param = json.loads(fin.read())\n",
    "net = HSINet(param)\n",
    "# x = torch.randn(4, 1,1200,13,13)\n",
    "# y = net(x) \n",
    "# print(y.shape)\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(net.cuda(), (1200, 13, 13), batch_size=128) # 1：batch_size 3:图片的通道数 224: 图片的高宽\n",
    "\n",
    "import torchinfo\n",
    "torchinfo.summary(net.cuda(), (1200, 13, 13), batch_dim = 0, col_names = (\"input_size\", \"output_size\",  \"kernel_size\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1235d-3aa3-4e95-a49e-852602a746af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
