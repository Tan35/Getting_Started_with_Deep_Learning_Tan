Hyperspectral data shape:  (145, 145, 200)
Label shape:  (145, 145)

... ... PCA tranformation ... ...
Hyperspectral data shape:  (145, 145, 200)
Label shape:  (145, 145)

... ... PCA tranformation ... ...
Data shape after PCA:  (145, 145, 30)

... ... create data cubes ... ...
Data shape after PCA:  (145, 145, 30)

... ... create data cubes ... ...
Data cube X shape:  (10249, 25, 25, 30)
Data cube y shape:  (10249,)

... ... create train & test data ... ...
Data cube X shape:  (10249, 25, 25, 30)
Data cube y shape:  (10249,)

... ... create train & test data ... ...
Xtrain shape:  (1024, 25, 25, 30)
Xtest  shape:  (9225, 25, 25, 30)
before transpose: Xtrain shape:  (1024, 25, 25, 30, 1)
before transpose: Xtest  shape:  (9225, 25, 25, 30, 1)
after transpose: Xtrain shape:  (1024, 1, 30, 25, 25)
after transpose: Xtest  shape:  (9225, 1, 30, 25, 25)
Cuda is available ✔
Xtrain shape:  (1024, 25, 25, 30)
Xtest  shape:  (9225, 25, 25, 30)
before transpose: Xtrain shape:  (1024, 25, 25, 30, 1)
before transpose: Xtest  shape:  (9225, 25, 25, 30, 1)
after transpose: Xtrain shape:  (1024, 1, 30, 25, 25)
after transpose: Xtest  shape:  (9225, 1, 30, 25, 25)
Cuda is available ✔
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Epoch: 1]   [loss avg: 11.7085]   [current loss: 2.9232]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
[Epoch: 1]   [loss avg: 12.3565]   [current loss: 3.3081]
[Epoch: 2]   [loss avg: 11.8815]   [current loss: 2.6481]
[Epoch: 2]   [loss avg: 11.4548]   [current loss: 2.2465]
[Epoch: 3]   [loss avg: 10.2841]   [current loss: 1.6988]
[Epoch: 3]   [loss avg: 10.6174]   [current loss: 1.8676]
[Epoch: 4]   [loss avg: 9.5526]   [current loss: 1.5290]
[Epoch: 4]   [loss avg: 9.2712]   [current loss: 1.4346]
[Epoch: 5]   [loss avg: 8.7804]   [current loss: 1.2757]
[Epoch: 5]   [loss avg: 8.4170]   [current loss: 1.1909]
[Epoch: 6]   [loss avg: 8.0977]   [current loss: 1.0525]
[Epoch: 6]   [loss avg: 7.7260]   [current loss: 1.0198]
[Epoch: 7]   [loss avg: 7.4997]   [current loss: 0.8485]
[Epoch: 7]   [loss avg: 7.0977]   [current loss: 0.7902]
[Epoch: 8]   [loss avg: 6.9758]   [current loss: 0.9082]
[Epoch: 8]   [loss avg: 6.5697]   [current loss: 0.6785]
[Epoch: 9]   [loss avg: 6.4812]   [current loss: 0.5800]
[Epoch: 9]   [loss avg: 6.0763]   [current loss: 0.5833]
[Epoch: 10]   [loss avg: 6.0330]   [current loss: 0.4110]
[Epoch: 10]   [loss avg: 5.6470]   [current loss: 0.5239]
[Epoch: 11]   [loss avg: 5.6194]   [current loss: 0.3208]
[Epoch: 11]   [loss avg: 5.2635]   [current loss: 0.3686]
[Epoch: 12]   [loss avg: 5.2556]   [current loss: 0.2820]
[Epoch: 12]   [loss avg: 4.9023]   [current loss: 0.2653]
[Epoch: 13]   [loss avg: 4.9282]   [current loss: 0.2312]
[Epoch: 13]   [loss avg: 4.5790]   [current loss: 0.2144]
[Epoch: 14]   [loss avg: 4.2894]   [current loss: 0.2012][Epoch: 14]   [loss avg: 4.6361]   [current loss: 0.1642]

[Epoch: 15]   [loss avg: 4.3740]   [current loss: 0.1744]
[Epoch: 15]   [loss avg: 4.0384]   [current loss: 0.1785]
[Epoch: 16]   [loss avg: 4.1309]   [current loss: 0.0906]
[Epoch: 16]   [loss avg: 3.8088]   [current loss: 0.1183]
[Epoch: 17]   [loss avg: 3.9164]   [current loss: 0.0813]
[Epoch: 17]   [loss avg: 3.6059]   [current loss: 0.1768]
[Epoch: 18]   [loss avg: 3.7187]   [current loss: 0.0687]
[Epoch: 18]   [loss avg: 3.4234]   [current loss: 0.0803]
[Epoch: 19]   [loss avg: 3.2548]   [current loss: 0.0452]
[Epoch: 19]   [loss avg: 3.5384]   [current loss: 0.0637]
[Epoch: 20]   [loss avg: 3.3732]   [current loss: 0.0466]
[Epoch: 20]   [loss avg: 3.1018]   [current loss: 0.0942]
[Epoch: 21]   [loss avg: 3.2273]   [current loss: 0.0529]
[Epoch: 21]   [loss avg: 2.9625]   [current loss: 0.0574]
[Epoch: 22]   [loss avg: 3.0903]   [current loss: 0.0789]
[Epoch: 22]   [loss avg: 2.8400]   [current loss: 0.1190]
[Epoch: 23]   [loss avg: 2.9696]   [current loss: 0.0362]
[Epoch: 23]   [loss avg: 2.7245]   [current loss: 0.0859]
[Epoch: 24]   [loss avg: 2.8560]   [current loss: 0.0670]
[Epoch: 24]   [loss avg: 2.6175]   [current loss: 0.0621]
[Epoch: 25]   [loss avg: 2.7491]   [current loss: 0.0196]
[Epoch: 25]   [loss avg: 2.5189]   [current loss: 0.0393]
[Epoch: 26]   [loss avg: 2.6494]   [current loss: 0.0225]
[Epoch: 26]   [loss avg: 2.4273]   [current loss: 0.0457]
[Epoch: 27]   [loss avg: 2.5589]   [current loss: 0.0424]
[Epoch: 27]   [loss avg: 2.3422]   [current loss: 0.0490]
[Epoch: 28]   [loss avg: 2.4726]   [current loss: 0.0716]
[Epoch: 28]   [loss avg: 2.2614]   [current loss: 0.0478]
[Epoch: 29]   [loss avg: 2.3921]   [current loss: 0.0224]
[Epoch: 29]   [loss avg: 2.1885]   [current loss: 0.0325]
[Epoch: 30]   [loss avg: 2.3150]   [current loss: 0.0179]
[Epoch: 30]   [loss avg: 2.1171]   [current loss: 0.0129]
[Epoch: 31]   [loss avg: 2.2422]   [current loss: 0.0251]
[Epoch: 31]   [loss avg: 2.0507]   [current loss: 0.0216]
[Epoch: 32]   [loss avg: 2.1751]   [current loss: 0.0612]
[Epoch: 32]   [loss avg: 1.9882]   [current loss: 0.0145]
[Epoch: 33]   [loss avg: 2.1122]   [current loss: 0.0097]
[Epoch: 33]   [loss avg: 1.9303]   [current loss: 0.0067]
[Epoch: 34]   [loss avg: 2.0520]   [current loss: 0.0197]
[Epoch: 34]   [loss avg: 1.8760]   [current loss: 0.0241]
[Epoch: 35]   [loss avg: 1.9951]   [current loss: 0.0130]
[Epoch: 35]   [loss avg: 1.8245]   [current loss: 0.0161]
[Epoch: 36]   [loss avg: 1.9417]   [current loss: 0.0166]
[Epoch: 36]   [loss avg: 1.7768]   [current loss: 0.0161]
[Epoch: 37]   [loss avg: 1.8917]   [current loss: 0.0182]
[Epoch: 37]   [loss avg: 1.7303]   [current loss: 0.0148]
[Epoch: 38]   [loss avg: 1.8435]   [current loss: 0.0101]
[Epoch: 38]   [loss avg: 1.6855]   [current loss: 0.0111]
[Epoch: 39]   [loss avg: 1.7980]   [current loss: 0.0040]
[Epoch: 39]   [loss avg: 1.6431]   [current loss: 0.0117]
[Epoch: 40]   [loss avg: 1.7563]   [current loss: 0.0320]
[Epoch: 40]   [loss avg: 1.6034]   [current loss: 0.0078]
[Epoch: 41]   [loss avg: 1.7148]   [current loss: 0.0042]
[Epoch: 41]   [loss avg: 1.5656]   [current loss: 0.0135]
[Epoch: 42]   [loss avg: 1.6756]   [current loss: 0.0165]
[Epoch: 42]   [loss avg: 1.5296]   [current loss: 0.0237]
[Epoch: 43]   [loss avg: 1.6391]   [current loss: 0.0202]
[Epoch: 43]   [loss avg: 1.4954]   [current loss: 0.0245]
[Epoch: 44]   [loss avg: 1.6045]   [current loss: 0.0345]
[Epoch: 44]   [loss avg: 1.4625]   [current loss: 0.0050]
[Epoch: 45]   [loss avg: 1.5704]   [current loss: 0.0032]
[Epoch: 45]   [loss avg: 1.4317]   [current loss: 0.0104]
[Epoch: 46]   [loss avg: 1.5379]   [current loss: 0.0158]
[Epoch: 46]   [loss avg: 1.4033]   [current loss: 0.0089]
[Epoch: 47]   [loss avg: 1.5072]   [current loss: 0.0175]
[Epoch: 47]   [loss avg: 1.3756]   [current loss: 0.0349]
[Epoch: 48]   [loss avg: 1.4767]   [current loss: 0.0085]
[Epoch: 48]   [loss avg: 1.3483]   [current loss: 0.0201]
[Epoch: 49]   [loss avg: 1.4483]   [current loss: 0.0183]
[Epoch: 49]   [loss avg: 1.3215]   [current loss: 0.0082]
[Epoch: 50]   [loss avg: 1.4202]   [current loss: 0.0078]
[Epoch: 50]   [loss avg: 1.2961]   [current loss: 0.0269]
[Epoch: 51]   [loss avg: 1.3948]   [current loss: 0.0362]
[Epoch: 51]   [loss avg: 1.2713]   [current loss: 0.0047]
[Epoch: 52]   [loss avg: 1.3694]   [current loss: 0.0073]
[Epoch: 52]   [loss avg: 1.2497]   [current loss: 0.0185]
[Epoch: 53]   [loss avg: 1.3448]   [current loss: 0.0239]
[Epoch: 53]   [loss avg: 1.2280]   [current loss: 0.0406]
[Epoch: 54]   [loss avg: 1.2062]   [current loss: 0.0138]
[Epoch: 54]   [loss avg: 1.3206]   [current loss: 0.0054]
[Epoch: 55]   [loss avg: 1.2973]   [current loss: 0.0090]
[Epoch: 55]   [loss avg: 1.1852]   [current loss: 0.0270]
[Epoch: 56]   [loss avg: 1.2744]   [current loss: 0.0049]
[Epoch: 56]   [loss avg: 1.1648]   [current loss: 0.0016]
[Epoch: 57]   [loss avg: 1.2537]   [current loss: 0.0142]
[Epoch: 57]   [loss avg: 1.1448]   [current loss: 0.0130]
[Epoch: 58]   [loss avg: 1.2332]   [current loss: 0.0038]
[Epoch: 58]   [loss avg: 1.1254]   [current loss: 0.0050]
[Epoch: 59]   [loss avg: 1.2129]   [current loss: 0.0079]
[Epoch: 59]   [loss avg: 1.1068]   [current loss: 0.0108]
[Epoch: 60]   [loss avg: 1.1930]   [current loss: 0.0027]
[Epoch: 60]   [loss avg: 1.0885]   [current loss: 0.0029]
[Epoch: 61]   [loss avg: 1.1736]   [current loss: 0.0038]
[Epoch: 61]   [loss avg: 1.0709]   [current loss: 0.0098]
[Epoch: 62]   [loss avg: 1.1550]   [current loss: 0.0083]
[Epoch: 62]   [loss avg: 1.0542]   [current loss: 0.0097]
[Epoch: 63]   [loss avg: 1.1374]   [current loss: 0.0007]
[Epoch: 63]   [loss avg: 1.0381]   [current loss: 0.0055]
[Epoch: 64]   [loss avg: 1.1198]   [current loss: 0.0038]
[Epoch: 64]   [loss avg: 1.0220]   [current loss: 0.0008]
[Epoch: 65]   [loss avg: 1.1031]   [current loss: 0.0104]
[Epoch: 65]   [loss avg: 1.0066]   [current loss: 0.0042]
[Epoch: 66]   [loss avg: 1.0866]   [current loss: 0.0023]
[Epoch: 66]   [loss avg: 0.9915]   [current loss: 0.0018]
[Epoch: 67]   [loss avg: 1.0705]   [current loss: 0.0013]
[Epoch: 67]   [loss avg: 0.9768]   [current loss: 0.0030]
[Epoch: 68]   [loss avg: 1.0549]   [current loss: 0.0060]
[Epoch: 68]   [loss avg: 0.9628]   [current loss: 0.0213]
[Epoch: 69]   [loss avg: 1.0401]   [current loss: 0.0167]
[Epoch: 69]   [loss avg: 0.9492]   [current loss: 0.0067]
[Epoch: 70]   [loss avg: 1.0261]   [current loss: 0.0448]
[Epoch: 70]   [loss avg: 0.9358]   [current loss: 0.0033]
[Epoch: 71]   [loss avg: 1.0118]   [current loss: 0.0004]
[Epoch: 71]   [loss avg: 0.9230]   [current loss: 0.0007]
[Epoch: 72]   [loss avg: 0.9980]   [current loss: 0.0024]
[Epoch: 72]   [loss avg: 0.9103]   [current loss: 0.0008]
[Epoch: 73]   [loss avg: 0.9850]   [current loss: 0.0048]
[Epoch: 73]   [loss avg: 0.8979]   [current loss: 0.0009]
[Epoch: 74]   [loss avg: 0.9718]   [current loss: 0.0002]
[Epoch: 74]   [loss avg: 0.8859]   [current loss: 0.0027]
[Epoch: 75]   [loss avg: 0.9592]   [current loss: 0.0086]
[Epoch: 75]   [loss avg: 0.8742]   [current loss: 0.0009]
[Epoch: 76]   [loss avg: 0.9472]   [current loss: 0.0240]
[Epoch: 76]   [loss avg: 0.8629]   [current loss: 0.0123]
[Epoch: 77]   [loss avg: 0.9351]   [current loss: 0.0009]
[Epoch: 77]   [loss avg: 0.8518]   [current loss: 0.0012]
[Epoch: 78]   [loss avg: 0.9240]   [current loss: 0.0019]
[Epoch: 78]   [loss avg: 0.8413]   [current loss: 0.0008]
[Epoch: 79]   [loss avg: 0.9127]   [current loss: 0.0138]
[Epoch: 79]   [loss avg: 0.8308]   [current loss: 0.0046]
[Epoch: 80]   [loss avg: 0.9015]   [current loss: 0.0006]
[Epoch: 80]   [loss avg: 0.8206]   [current loss: 0.0035]
[Epoch: 81]   [loss avg: 0.8910]   [current loss: 0.0101]
[Epoch: 81]   [loss avg: 0.8106]   [current loss: 0.0018]
[Epoch: 82]   [loss avg: 0.8806]   [current loss: 0.0113]
[Epoch: 82]   [loss avg: 0.8010]   [current loss: 0.0162]
[Epoch: 83]   [loss avg: 0.8701]   [current loss: 0.0009]
[Epoch: 83]   [loss avg: 0.7915]   [current loss: 0.0026]
[Epoch: 84]   [loss avg: 0.8598]   [current loss: 0.0016]
[Epoch: 84]   [loss avg: 0.7822]   [current loss: 0.0011]
[Epoch: 85]   [loss avg: 0.7734]   [current loss: 0.0287]
[Epoch: 85]   [loss avg: 0.8498]   [current loss: 0.0020]
[Epoch: 86]   [loss avg: 0.8403]   [current loss: 0.0177]
[Epoch: 86]   [loss avg: 0.7646]   [current loss: 0.0016]
[Epoch: 87]   [loss avg: 0.8311]   [current loss: 0.0086]
[Epoch: 87]   [loss avg: 0.7561]   [current loss: 0.0035]
[Epoch: 88]   [loss avg: 0.8221]   [current loss: 0.0252]
[Epoch: 88]   [loss avg: 0.7476]   [current loss: 0.0036]
[Epoch: 89]   [loss avg: 0.7394]   [current loss: 0.0120]
[Epoch: 89]   [loss avg: 0.8138]   [current loss: 0.0014]
[Epoch: 90]   [loss avg: 0.7318]   [current loss: 0.0083]
[Epoch: 90]   [loss avg: 0.8051]   [current loss: 0.0153]
[Epoch: 91]   [loss avg: 0.7969]   [current loss: 0.0126]
[Epoch: 91]   [loss avg: 0.7243]   [current loss: 0.0160]
[Epoch: 92]   [loss avg: 0.7885]   [current loss: 0.0086]
[Epoch: 92]   [loss avg: 0.7165]   [current loss: 0.0017]
[Epoch: 93]   [loss avg: 0.7803]   [current loss: 0.0028]
[Epoch: 93]   [loss avg: 0.7090]   [current loss: 0.0031]
[Epoch: 94]   [loss avg: 0.7723]   [current loss: 0.0056]
[Epoch: 94]   [loss avg: 0.7020]   [current loss: 0.0004]
[Epoch: 95]   [loss avg: 0.7643]   [current loss: 0.0019]
[Epoch: 95]   [loss avg: 0.6947]   [current loss: 0.0014]
[Epoch: 96]   [loss avg: 0.7563]   [current loss: 0.0006]
[Epoch: 96]   [loss avg: 0.6876]   [current loss: 0.0007]
[Epoch: 97]   [loss avg: 0.7488]   [current loss: 0.0014]
[Epoch: 97]   [loss avg: 0.6806]   [current loss: 0.0026]
[Epoch: 98]   [loss avg: 0.7413]   [current loss: 0.0021]
[Epoch: 98]   [loss avg: 0.6738]   [current loss: 0.0065]
[Epoch: 99]   [loss avg: 0.7340]   [current loss: 0.0076]
[Epoch: 99]   [loss avg: 0.6672]   [current loss: 0.0004]
[Epoch: 100]   [loss avg: 0.7272]   [current loss: 0.0383]
Training time: 30.383486032485962
Finished Training
[Epoch: 100]   [loss avg: 0.6616]   [current loss: 0.0004]
Training time: 30.396994829177856
Finished Training
              precision    recall  f1-score   support

         0.0     1.0000    0.9268    0.9620        41
         1.0     0.9943    0.9455    0.9693      1285
         2.0     0.9894    1.0000    0.9947       747
         3.0     0.9952    0.9765    0.9858       213
         4.0     0.9886    0.9954    0.9920       435
         5.0     0.9924    0.9939    0.9932       657
         6.0     1.0000    1.0000    1.0000        25
         7.0     0.9773    1.0000    0.9885       430
         8.0     0.9444    0.9444    0.9444        18
         9.0     0.9841    0.9920    0.9880       875
        10.0     0.9768    0.9905    0.9836      2210
        11.0     0.9700    0.9682    0.9691       534
        12.0     1.0000    0.9838    0.9918       185
        13.0     0.9982    0.9939    0.9960      1139
        14.0     0.9747    1.0000    0.9872       347
        15.0     0.8617    0.9643    0.9101        84

    accuracy                         0.9845      9225
   macro avg     0.9779    0.9797    0.9785      9225
weighted avg     0.9848    0.9845    0.9845      9225

              precision    recall  f1-score   support

         0.0     1.0000    1.0000    1.0000        41
         1.0     0.9934    0.9432    0.9677      1285
         2.0     0.9894    1.0000    0.9947       747
         3.0     0.9952    0.9765    0.9858       213
         4.0     0.9908    0.9954    0.9931       435
         5.0     0.9924    0.9939    0.9932       657
         6.0     1.0000    1.0000    1.0000        25
         7.0     0.9908    1.0000    0.9954       430
         8.0     0.9474    1.0000    0.9730        18
         9.0     0.9841    0.9920    0.9880       875
        10.0     0.9751    0.9928    0.9839      2210
        11.0     0.9718    0.9663    0.9690       534
        12.0     1.0000    0.9838    0.9918       185
        13.0     0.9991    0.9930    0.9960      1139
        14.0     0.9775    1.0000    0.9886       347
        15.0     0.8526    0.9643    0.9050        84

    accuracy                         0.9849      9225
   macro avg     0.9787    0.9876    0.9828      9225
weighted avg     0.9852    0.9849    0.9849      9225

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
