{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9118691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import time, json\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange, repeat\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "# 等于 PreNorm\n",
    "class LayerNormalize(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "# 等于 FeedForward\n",
    "class MLP_Block(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, heads, dim_heads, dropout):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_heads * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_heads ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, mask = None):\n",
    "        # x:[b,n,dim]\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        # get qkv tuple:([b,n,head_num*head_dim],[...],[...])\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        # split q,k,v from [b,n,head_num*head_dim] -> [b,head_num,n,head_dim]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        # transpose(k) * q / sqrt(head_dim) -> [b,head_num,n,n]\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "\n",
    "        # mask value: -inf\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        # softmax normalization -> attention matrix\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        # value * attention matrix -> output\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        # cat all output -> [b, n, head_num*head_dim]\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(LayerNormalize(dim, Attention(dim, heads=heads, dim_heads=dim_heads, dropout=dropout))),\n",
    "                Residual(LayerNormalize(dim, MLP_Block(dim, mlp_dim, dropout=dropout)))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print('i am here Transformer')\n",
    "        for attention, mlp in self.layers:\n",
    "            x = attention(x, mask=mask)  # go to attention\n",
    "            x = mlp(x)  # go to MLP_Block\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chnls, ratio):\n",
    "        super(SE, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.compress = nn.Conv2d(in_chnls, in_chnls//ratio, 1, 1, 0)\n",
    "        self.excitation = nn.Conv2d(in_chnls//ratio, in_chnls, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.squeeze(x)\n",
    "        out = self.compress(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.excitation(out)\n",
    "        return torch.sigmoid(out) # 2023.09.21 运行这里报错了 原来是 F.sigmoid(out)\n",
    "\n",
    "\n",
    "\n",
    "class HSINet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HSINet, self).__init__()\n",
    "        self.params = params\n",
    "        net_params = params['net']\n",
    "        data_params = params['data']\n",
    "\n",
    "        num_classes = data_params.get(\"num_classes\", 16)\n",
    "        patch_size = data_params.get(\"patch_size\", 13)\n",
    "        self.spectral_size = data_params.get(\"spectral_size\", 200)\n",
    "\n",
    "        depth = net_params.get(\"depth\", 1)\n",
    "        heads = net_params.get(\"heads\", 8)\n",
    "        mlp_dim = net_params.get(\"mlp_dim\", 8)\n",
    "        dropout = net_params.get(\"dropout\", 0)\n",
    "        conv2d_out = 64\n",
    "        dim = net_params.get(\"dim\", 64)\n",
    "        dim_heads = dim\n",
    "        mlp_head_dim = dim\n",
    "        \n",
    "        image_size = patch_size * patch_size\n",
    "\n",
    "        self.pixel_patch_embedding = nn.Linear(conv2d_out, dim)\n",
    "\n",
    "        self.local_trans_pixel = Transformer(dim=dim, depth=depth, heads=heads, dim_heads=dim_heads, mlp_dim=mlp_dim, dropout=dropout)\n",
    "        self.new_image_size = image_size\n",
    "        self.pixel_pos_embedding = nn.Parameter(torch.randn(1, self.new_image_size+1, dim))\n",
    "        self.pixel_pos_scale = nn.Parameter(torch.ones(1) * 0.01)\n",
    "\n",
    "        self.conv2d_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.spectral_size, out_channels=conv2d_out, kernel_size=(3, 3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(conv2d_out),\n",
    "            nn.ReLU(),\n",
    "            # featuremap 是在这之后加一层channel上的压缩\n",
    "            # nn.Conv2d(in_channels=conv2d_out,out_channels=dim,kernel_size=3,padding=1),\n",
    "            # nn.BatchNorm2d(dim),\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "\n",
    "#         self.senet = SE(conv2d_out, 5)\n",
    "\n",
    "        self.cls_token_pixel = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.to_latent_pixel = nn.Identity()\n",
    "\n",
    "        self.mlp_head =nn.Linear(dim, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.mlp_head.weight)\n",
    "        torch.nn.init.normal_(self.mlp_head.bias, std=1e-6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        linear_dim = dim * 2\n",
    "        self.classifier_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, linear_dim),\n",
    "            nn.BatchNorm1d(linear_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(linear_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def encoder_block(self, x):\n",
    "        '''\n",
    "        x: (batch, s, w, h), s=spectral, w=weigth, h=height\n",
    "        '''\n",
    "        print(f'x.shape1 is {x.shape}')\n",
    "        x_pixel = x \n",
    "\n",
    "        b, s, w, h = x_pixel.shape\n",
    "        img = w * h\n",
    "        x_pixel = self.conv2d_features(x_pixel)\n",
    "        print(f'x.shape2 is {x_pixel.shape}')\n",
    "\n",
    "\n",
    "#         scale = self.senet(x_pixel)\n",
    "#         print(f'x.shape3 is {scale.shape}')\n",
    "        # print('scale shape is ', scale.shape)\n",
    "        # print('pixel shape is ', x_pixel.shape)\n",
    "        # x_pixel = x_pixel * scale#(batch, image_size, dim)\n",
    "\n",
    "        #1. reshape\n",
    "        x_pixel = rearrange(x_pixel, 'b s w h-> b (w h) s') # (batch, w*h, s)\n",
    "        print(f'x.shape4 is {x_pixel.shape}')\n",
    "\n",
    "        #2. patch_embedding\n",
    "        # x_pixel = self.pixel_patch_embedding(x_pixel)\n",
    "\n",
    "        #3. local transformer\n",
    "        cls_tokens_pixel = self.cls_token_pixel.expand(x_pixel.shape[0], -1, -1)\n",
    "        print(f'x.shape5 is {cls_tokens_pixel.shape}')\n",
    "        x_pixel = torch.cat((cls_tokens_pixel, x_pixel), dim = 1) #[b,image+1,dim]\n",
    "        print(f'x.shape6 is {x_pixel.shape}')\n",
    "        x_pixel = x_pixel + self.pixel_pos_embedding[:,:] * self.pixel_pos_scale\n",
    "        print(f'x.shape7 is {x_pixel.shape}')\n",
    "        # x_pixel = x_pixel + self.pixel_pos_embedding[:,:] \n",
    "        # x_pixel = self.dropout(x_pixel)\n",
    "\n",
    "        x_pixel = self.local_trans_pixel(x_pixel) #(batch, image_size+1, dim)\n",
    "        print(f'x.shape8 is {x_pixel.shape}')\n",
    "\n",
    "#         out = (torch.bmm(out, torch.transpose(out, 1, 2)) / feature_size).view(batch_size, -1) #b, 4096\n",
    "        \n",
    "        logit_pixel = self.to_latent_pixel(x_pixel[:,0])\n",
    "        print(f'x.shape9 is {logit_pixel.shape}')\n",
    "        \n",
    "    \n",
    "        \n",
    "        logit_x = logit_pixel \n",
    "        reduce_x = torch.mean(x_pixel, dim=1)\n",
    "        \n",
    "        return logit_x, reduce_x\n",
    "\n",
    "    def forward(self, x,left=None,right=None):\n",
    "        '''\n",
    "        x: (batch, s, w, h), s=spectral, w=weigth, h=height\n",
    "\n",
    "        '''\n",
    "        logit_x, _ = self.encoder_block(x)\n",
    "#         print(f'logit_x shape is {logit_x.shape}')\n",
    "        mean_left, mean_right = None, None\n",
    "        if left is not None and right is not None:\n",
    "            _, mean_left = self.encoder_block(left)\n",
    "            _, mean_right = self.encoder_block(right)\n",
    "\n",
    "        # return  self.mlp_head(logit_x), mean_left, mean_right \n",
    "        return  self.classifier_mlp(logit_x), mean_left, mean_right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0f7f93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape1 is torch.Size([1, 1200, 13, 13])\n",
      "x.shape2 is torch.Size([1, 64, 13, 13])\n",
      "x.shape4 is torch.Size([1, 169, 64])\n",
      "x.shape5 is torch.Size([1, 1, 64])\n",
      "x.shape6 is torch.Size([1, 170, 64])\n",
      "x.shape7 is torch.Size([1, 170, 64])\n",
      "i am here Transformer\n",
      "x.shape8 is torch.Size([1, 170, 64])\n",
      "x.shape9 is torch.Size([1, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Kernel Shape\n",
       "==================================================================================================================================\n",
       "HSINet                                                  [1, 1200, 13, 13]         [1, 16]                   --\n",
       "├─Sequential: 1-1                                       [1, 1200, 13, 13]         [1, 64, 13, 13]           --\n",
       "│    └─Conv2d: 2-1                                      [1, 1200, 13, 13]         [1, 64, 13, 13]           [3, 3]\n",
       "│    └─BatchNorm2d: 2-2                                 [1, 64, 13, 13]           [1, 64, 13, 13]           --\n",
       "│    └─ReLU: 2-3                                        [1, 64, 13, 13]           [1, 64, 13, 13]           --\n",
       "├─Transformer: 1-2                                      [1, 170, 64]              [1, 170, 64]              --\n",
       "│    └─ModuleList: 2-4                                  --                        --                        --\n",
       "│    │    └─ModuleList: 3-1                             --                        --                        --\n",
       "│    │    └─ModuleList: 3-2                             --                        --                        --\n",
       "├─Identity: 1-3                                         [1, 64]                   [1, 64]                   --\n",
       "├─Sequential: 1-4                                       [1, 64]                   [1, 16]                   --\n",
       "│    └─Linear: 2-5                                      [1, 64]                   [1, 128]                  --\n",
       "│    └─BatchNorm1d: 2-6                                 [1, 128]                  [1, 128]                  --\n",
       "│    └─Dropout: 2-7                                     [1, 128]                  [1, 128]                  --\n",
       "│    └─ReLU: 2-8                                        [1, 128]                  [1, 128]                  --\n",
       "│    └─Linear: 2-9                                      [1, 128]                  [1, 16]                   --\n",
       "==================================================================================================================================\n",
       "Total params: 1,376,369\n",
       "Trainable params: 1,376,369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 117.49\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.81\n",
       "Forward/backward pass size (MB): 11.34\n",
       "Params size (MB): 5.44\n",
       "Estimated Total Size (MB): 17.59\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_param = './params/indian_diffusion.json'\n",
    "with open(path_param, 'r') as fin:\n",
    "    param = json.loads(fin.read())\n",
    "net = HSINet(param)\n",
    "# x = torch.randn(4, 1,1200,13,13)\n",
    "# y = net(x) \n",
    "# print(y.shape)\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(net.cuda(), (1200, 13, 13), batch_size=128) # 1：batch_size 3:图片的通道数 224: 图片的高宽\n",
    "\n",
    "import torchinfo\n",
    "torchinfo.summary(net.cuda(), (1200, 13, 13), batch_dim = 0, col_names = (\"input_size\", \"output_size\",  \"kernel_size\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c677156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSINet(\n",
      "  (pixel_patch_embedding): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (local_trans_pixel): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Residual(\n",
      "          (fn): LayerNormalize(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (to_qkv): Linear(in_features=64, out_features=3840, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=64, bias=True)\n",
      "                (1): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Residual(\n",
      "          (fn): LayerNormalize(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): MLP_Block(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=64, out_features=8, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0, inplace=False)\n",
      "                (3): Linear(in_features=8, out_features=64, bias=True)\n",
      "                (4): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Residual(\n",
      "          (fn): LayerNormalize(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (to_qkv): Linear(in_features=64, out_features=3840, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=1280, out_features=64, bias=True)\n",
      "                (1): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Residual(\n",
      "          (fn): LayerNormalize(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): MLP_Block(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=64, out_features=8, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0, inplace=False)\n",
      "                (3): Linear(in_features=8, out_features=64, bias=True)\n",
      "                (4): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2d_features): Sequential(\n",
      "    (0): Conv2d(1200, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (senet): SE(\n",
      "    (squeeze): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (compress): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (excitation): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (to_latent_pixel): Identity()\n",
      "  (mlp_head): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier_mlp): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef78c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 64, 13, 13]         691,264\n",
      "       BatchNorm2d-2          [128, 64, 13, 13]             128\n",
      "              ReLU-3          [128, 64, 13, 13]               0\n",
      " AdaptiveAvgPool2d-4            [128, 64, 1, 1]               0\n",
      "            Conv2d-5            [128, 12, 1, 1]             780\n",
      "            Conv2d-6            [128, 64, 1, 1]             832\n",
      "                SE-7            [128, 64, 1, 1]               0\n",
      "         LayerNorm-8             [128, 170, 64]             128\n",
      "            Linear-9           [128, 170, 3840]         245,760\n",
      "           Linear-10             [128, 170, 64]          81,984\n",
      "          Dropout-11             [128, 170, 64]               0\n",
      "        Attention-12             [128, 170, 64]               0\n",
      "   LayerNormalize-13             [128, 170, 64]               0\n",
      "         Residual-14             [128, 170, 64]               0\n",
      "        LayerNorm-15             [128, 170, 64]             128\n",
      "           Linear-16              [128, 170, 8]             520\n",
      "             GELU-17              [128, 170, 8]               0\n",
      "          Dropout-18              [128, 170, 8]               0\n",
      "           Linear-19             [128, 170, 64]             576\n",
      "          Dropout-20             [128, 170, 64]               0\n",
      "        MLP_Block-21             [128, 170, 64]               0\n",
      "   LayerNormalize-22             [128, 170, 64]               0\n",
      "         Residual-23             [128, 170, 64]               0\n",
      "        LayerNorm-24             [128, 170, 64]             128\n",
      "           Linear-25           [128, 170, 3840]         245,760\n",
      "           Linear-26             [128, 170, 64]          81,984\n",
      "          Dropout-27             [128, 170, 64]               0\n",
      "        Attention-28             [128, 170, 64]               0\n",
      "   LayerNormalize-29             [128, 170, 64]               0\n",
      "         Residual-30             [128, 170, 64]               0\n",
      "        LayerNorm-31             [128, 170, 64]             128\n",
      "           Linear-32              [128, 170, 8]             520\n",
      "             GELU-33              [128, 170, 8]               0\n",
      "          Dropout-34              [128, 170, 8]               0\n",
      "           Linear-35             [128, 170, 64]             576\n",
      "          Dropout-36             [128, 170, 64]               0\n",
      "        MLP_Block-37             [128, 170, 64]               0\n",
      "   LayerNormalize-38             [128, 170, 64]               0\n",
      "         Residual-39             [128, 170, 64]               0\n",
      "      Transformer-40             [128, 170, 64]               0\n",
      "         Identity-41                  [128, 64]               0\n",
      "           Linear-42                 [128, 128]           8,320\n",
      "      BatchNorm1d-43                 [128, 128]             256\n",
      "          Dropout-44                 [128, 128]               0\n",
      "             ReLU-45                 [128, 128]               0\n",
      "           Linear-46                  [128, 16]           2,064\n",
      "================================================================\n",
      "Total params: 1,361,836\n",
      "Trainable params: 1,361,836\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 99.02\n",
      "Forward/backward pass size (MB): 1581.06\n",
      "Params size (MB): 5.19\n",
      "Estimated Total Size (MB): 1685.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net.cuda(), (1200, 13, 13), batch_size=128) # 1：batch_size 3:图片的通道数 224: 图片的高宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92cb7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape1 is torch.Size([128, 1200, 13, 13])\n",
      "x.shape2 is torch.Size([128, 64, 13, 13])\n",
      "x.shape3 is torch.Size([128, 64, 1, 1])\n",
      "x.shape4 is torch.Size([128, 169, 64])\n",
      "x.shape5 is torch.Size([128, 1, 64])\n",
      "x.shape6 is torch.Size([128, 170, 64])\n",
      "x.shape7 is torch.Size([128, 170, 64])\n",
      "i am here Transformer\n",
      "x.shape8 is torch.Size([128, 170, 64])\n",
      "type is torch.Size([128, 64])\n",
      "x.shape9 is torch.Size([128, 64])\n",
      "torch.Size([128, 16])\n"
     ]
    }
   ],
   "source": [
    "path_param = './params/indian_diffusion.json'\n",
    "with open(path_param, 'r') as fin:\n",
    "    param = json.loads(fin.read())\n",
    "net2 = HSINet(param).cuda()\n",
    "x = torch.randn(128,1200,13,13).cuda()\n",
    "y = net2(x)\n",
    "print(y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e7603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test77",
   "language": "python",
   "name": "test77"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
